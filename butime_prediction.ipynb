{"cells":[{"cell_type":"code","source":["total_df = spark.read.json('/mnt/dacoursedatabricksstg/dacoursedatabricksdata/busFile')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["total_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: 237143372</div>"]}}],"execution_count":2},{"cell_type":"code","source":["total_df = total_df[total_df.atStop == True]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["total_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: 50660852</div>"]}}],"execution_count":4},{"cell_type":"code","source":["total_df = total_df[['_id','atStop','busStop','congestion','delay','justLeftStop','justStopped','latitude' ,'longitude','vehicleId','vehicleSpeed','actualDelay','timestamp','journeyPatternId']]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["#correct original data and add new fields\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import LongType,TimestampType,StringType,DateType\n\ndef get_weekday(s):\n  return s.weekday()\n\ndef get_hour(s):\n  return s.hour\n\ndef get_is_weekend(s):\n  return 1 if s in [6,7] else 0\n\ndef get_id(s):\n  return s['$oid']\n\ndef get_month(s):\n  return s.month\n\ndef get_only_date(s):\n  return str(s.date())\n\ndef get_just_left(s):\n  return 1 if s else 0\n\ndef get_just_stopped(s):\n  return 1 if s else 0\n\ndef get_congestion(s):\n  return 1 if s else 0\n\ndef get_timestamp(s):\n    return int(s['$numberLong'][:-3])\n\nget_timestamp_udf = udf(get_timestamp,LongType())\nget_weekday_udf = udf(get_weekday, LongType())\nget_hour_udf = udf(get_hour, LongType())\nget_is_weekend_udf = udf(get_is_weekend, LongType())\nget_id_udf = udf(get_id, StringType())\nget_month_udf = udf(get_month, LongType())\nget_only_date_udf = udf(get_only_date, StringType())\nget_just_left_udf = udf(get_just_left, LongType())\nget_just_stopped_udf = udf(get_just_stopped, LongType())\nget_congestion_udf = udf(get_congestion, LongType())\n\n\ntotal_df = total_df.withColumn('date', get_timestamp_udf('timestamp').cast('timestamp'))\ntotal_df = total_df.withColumn('weekday', get_weekday_udf('date'))\ntotal_df = total_df.withColumn('month', get_month_udf('date'))\ntotal_df = total_df.withColumn('only_date', get_only_date_udf('date'))\ntotal_df = total_df.withColumn('is_weekend', get_is_weekend_udf('weekday'))\ntotal_df = total_df.withColumn('hour', get_hour_udf('date'))\ntotal_df = total_df.withColumn('id', get_id_udf('_id'))\ntotal_df = total_df.withColumn('just_left', get_just_left_udf('justLeftStop'))\ntotal_df = total_df.withColumn('just_stopped', get_just_stopped_udf('justStopped'))\ntotal_df = total_df.withColumn('conges', get_just_left_udf('congestion'))\ntotal_df = total_df.drop(*['_id','atStop','justLeftStop','justStopped','congestion','timestamp'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import *\nimport pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import lit\n\n# small_list = total_df.take(1000)\n# small_df = spark.createDataFrame(small_list, total_df.columns)\n\n#keep only transitions between stops\nmy_window = Window.partitionBy('vehicleId').orderBy('date')\nlearn_df = total_df.withColumn('origin', f.lag(total_df.busStop.cast(\"bigint\")).over(my_window))\nlearn_df = learn_df[learn_df['busStop'] != learn_df['origin']]\n\n#calculate time difference between stops\nlearn_df = learn_df.withColumn('prev_ts', f.lag(learn_df.date.cast(\"bigint\")).over(my_window))\nlearn_df = learn_df.withColumn('time_to_reach_next', f.when(f.isnull(learn_df.date.cast(\"bigint\") - learn_df.prev_ts), 0)\n                            .otherwise(learn_df.date.cast(\"bigint\") - learn_df.prev_ts))\nlearn_df = learn_df[ learn_df['time_to_reach_next'] != 0]\n\n#transform into training data\nlearn_df = learn_df.withColumnRenamed('busStop', 'dest')\nlearn_df = learn_df.drop('prev_ts')\n\n#add field of from->to\ndef get_from_to(frm,to):\n  return str(frm)+'->'+str(to)\n\nget_from_to_udf = udf(get_from_to, StringType())\nlearn_df = learn_df.withColumn('from_to', lit(get_from_to_udf(f.col('origin'),f.col('dest'))))\n\nw = Window.partitionBy('from_to')\nlearn_df = learn_df.withColumn('seg_count', f.count('from_to').over(w))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["learn_df = learn_df[learn_df.time_to_reach_next < 7200] #drop rows with more then two hours between stops\nlearn_df = learn_df[learn_df.time_to_reach_next > 30]  #drop rows with less then 30 seconds between stops\nlearn_df = learn_df.filter(learn_df.seg_count > 100) #take only comoon segments \nlearn_df = learn_df.filter(learn_df.date < lit('2018-08-01')) #save last two months of the data for testing in streaming (August\\July 2018)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["learn_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[71]: 27420901</div>"]}}],"execution_count":9},{"cell_type":"code","source":["my_window = Window.partitionBy('from_to')\n#mean time segment\nlearn_df = learn_df.withColumn('mean_seg', f.mean('time_to_reach_next').over(my_window))\n#variance in segment\nlearn_df = learn_df.withColumn('stdev_seg', f.stddev('time_to_reach_next').over(my_window))\n\n#remove outlayres, greater than 3 stdevs from avg or or less than 2 stdevs \nlearn_df = learn_df[learn_df.time_to_reach_next < (learn_df.mean_seg + 3 * learn_df.stdev_seg)]\nlearn_df = learn_df[learn_df.time_to_reach_next > (learn_df.mean_seg - 3 * learn_df.stdev_seg)]\n\n#max time segment\nlearn_df = learn_df.withColumn('max_in_seg', f.max('time_to_reach_next').over(my_window))\n\n#min time segment\nlearn_df = learn_df.withColumn('min_in_seg', f.min('time_to_reach_next').over(my_window))\n\n#prev_time_in_segment\nmy_window = Window.partitionBy('from_to').orderBy('date')\nlearn_df = learn_df.withColumn('prev_time', f.lag(learn_df.time_to_reach_next).over(my_window))\n\n#calculate distances\nmy_window = Window.partitionBy('vehicleId').orderBy('date')\nlearn_df = learn_df.withColumn('prev_lat', f.lag(learn_df.latitude).over(my_window))\nlearn_df = learn_df.withColumn('prev_lon', f.lag(learn_df.longitude).over(my_window))   \nlearn_df = learn_df.na.fill(0)\nlearn_df = learn_df[learn_df['prev_lat'] != 0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["from math import radians, cos, sin, asin, sqrt\n\n@f.udf(\"float\")\ndef get_distance(longit_a, latit_a, longit_b, latit_b):\n    if None in [longit_a, latit_a, longit_b, latit_b]:\n        return 9999\n    # Transform to radians\n    longit_a, latit_a, longit_b, latit_b = map(radians, [longit_a,  latit_a, longit_b, latit_b])\n    dist_longit = longit_b - longit_a\n    dist_latit = latit_b - latit_a\n    # Calculate area\n    area = sin(dist_latit/2)**2 + cos(latit_a) * cos(latit_b) * sin(dist_longit/2)**2\n    # Calculate the central angle\n    central_angle = 2 * asin(sqrt(area))\n    radius = 6371 # THIS IN KM\n    # Calculate Distance\n    distance = central_angle * radius\n    return distance * 1000\n  \n@f.udf(\"float\")\ndef get_dis_from_center(longit_a, latit_a):\n  longit_b, latit_b = 53.3422665, -6.2554468 #city center coordinates\n  if None in [longit_a, latit_a, longit_b, latit_b]:\n      return 9999\n  # Transform to radians\n  longit_a, latit_a, longit_b, latit_b = map(radians, [longit_a,  latit_a, longit_b, latit_b])\n  dist_longit = longit_b - longit_a\n  dist_latit = latit_b - latit_a\n  # Calculate area\n  area = sin(dist_latit/2)**2 + cos(latit_a) * cos(latit_b) * sin(dist_longit/2)**2\n  # Calculate the central angle\n  central_angle = 2 * asin(sqrt(area))\n  radius = 6371 # THIS IN KM\n  # Calculate Distance\n  distance = central_angle * radius\n  return distance * 1000\n\n\nlearn_df = learn_df.withColumn('distnce_between',get_distance(f.col('prev_lat'),f.col('prev_lon'),f.col('latitude'),f.col('longitude')))\nlearn_df = learn_df.withColumn('lat_between',(learn_df.prev_lat + learn_df.latitude)/ 2)\nlearn_df = learn_df.withColumn('lon_between',(learn_df.prev_lon + learn_df.longitude)/ 2)\nlearn_df = learn_df.withColumn('dis_from_center',get_dis_from_center(f.col('lat_between'),f.col('lon_between')))\nlearn_df = learn_df[learn_df['distnce_between'] < 100000]\nlearn_df = learn_df[learn_df['dis_from_center'] < 100000]\nlearn_df = learn_df.withColumn('speed_in_seg',learn_df.distnce_between / learn_df.time_to_reach_next)\nlearn_df = learn_df[learn_df['speed_in_seg'] < 55] #drop segments with speed over 200kmh\nlearn_df = learn_df.drop('prev_lat','prev_lon','lat_between','lon_between')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["#count specific attributes from origin to dest\ncols = ['journeyPatternId', 'origin','dest']\nw = Window.partitionBy(cols)\nranked =  learn_df.withColumn('specific', f.count('dest').over(w))\nranked = ranked.withColumn('avg_ts', f.mean('date').over(w))\n\n#count total attributes from origin\ncols = ['journeyPatternId', 'origin']\nw = Window.partitionBy(cols)\nranked =  ranked.withColumn('total', f.count('origin').over(w))\n\n#calculate precentege of each option of 'origin->dest'\nranked = ranked.withColumn('precentege', f.col('specific') / f.col('total'))\n\n#create table to store true paths of journy id and origin\ntrue_paths = ranked.groupBy('journeyPatternId','origin','dest').max('precentege','avg_ts')\ncols = ['journeyPatternId', 'origin']\nw = Window.partitionBy(cols)#.orderBy(count('dest'))\ntrue_paths =  true_paths.withColumn('top_score', f.max('max(precentege)').over(w))\ntrue_paths = true_paths[true_paths['max(precentege)'] > 0.25] #keep only common transitions (over 25%)\ntrue_paths =  true_paths.withColumn('most_relevent_ts', f.max('max(avg_ts)').over(w))\n\ntrue_paths = true_paths[true_paths['most_relevent_ts'] == true_paths['max(avg_ts)']]\ntrue_paths = true_paths.withColumnRenamed('max(precentege)','seg_score')\ntrue_paths = true_paths.drop('max(avg_ts)','most_relevent_ts')\n\n# true_paths.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# true_paths.write.saveAsTable(\"true_paths_v2\")\ntrue_paths = spark.read.options(header='true', inferSchema = 'true').load('/user/hive/warehouse/true_paths_v2')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["small_list = learn_df.take(100000)\nsmall_df = spark.createDataFrame(small_list, learn_df.columns)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["import six\nfor i in small_df.drop('date').columns:\n    if not( isinstance(small_df.select(i).take(1)[0][0], six.string_types)):\n        print( \"Correlation for \", i, small_df.stat.corr('time_to_reach_next',i))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Correlation for  dest 0.05594198891723634\nCorrelation for  delay -0.07634012407102733\nCorrelation for  latitude -0.00675804219565852\nCorrelation for  longitude 0.004594914937379602\nCorrelation for  vehicleId -0.03167207654126484\nCorrelation for  vehicleSpeed 0.0024016272490318858\nCorrelation for  actualDelay -0.08361287950474948\nCorrelation for  weekday -0.016959940805643985\nCorrelation for  month -0.005649750000447006\nCorrelation for  is_weekend -0.024319883344506958\nCorrelation for  hour 0.025092627202521152\nCorrelation for  just_left nan\nCorrelation for  just_stopped -0.04532733553065105\nCorrelation for  conges nan\nCorrelation for  origin 0.168215974886636\nCorrelation for  time_to_reach_next 1.0\nCorrelation for  seg_count -0.11429906980919442\nCorrelation for  mean_seg 0.8447337021418053\nCorrelation for  stdev_seg 0.703113679934722\nCorrelation for  max_in_seg 0.7807651331359851\nCorrelation for  min_in_seg 0.4848906847493631\nCorrelation for  prev_time 0.7373435431240561\nCorrelation for  distnce_between 0.3376909575909145\nCorrelation for  dis_from_center -0.006958855460701354\nCorrelation for  speed_in_seg -0.172966626708094\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nsmall_pandas = small_df.toPandas()\n# plt.matshow(small_pandas.corr())\n# display(plt.show())\nsmall_pandas = small_pandas.drop(['vehicleId','only_date','id','journeyPatternId','just_stopped','hour','is_weekend','just_left','month','weekday','date','conges','from_to','seg_count'], axis = 1)\nf = plt.figure(figsize=(10, 10))\nplt.matshow(small_pandas.corr(), fignum=f.number)\nplt.xticks(range(small_pandas.shape[1]), small_pandas.columns, fontsize=8, rotation=45)\nplt.yticks(range(small_pandas.shape[1]), small_pandas.columns, fontsize=8)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\ndf = learn_df\n\nnumericCols =['delay','vehicleSpeed','actualDelay','max_in_seg','min_in_seg','stdev_seg','distnce_between','dis_from_center','hour','speed_in_seg','latitude','longitude','mean_seg','prev_time']\nassembler = VectorAssembler(inputCols = numericCols, outputCol=\"features\")\ndf = assembler.transform(df)\ndf = df.select([ 'features','time_to_reach_next'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: 25038683</div>"]}}],"execution_count":18},{"cell_type":"code","source":["train, test = df.randomSplit([0.99, 0.01])\n# print(\"Training Dataset Count: \" + str(train.count()))\n# print(\"Test Dataset Count: \" + str(test.count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nlr = LinearRegression(featuresCol = 'features', labelCol='time_to_reach_next', maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model = lr.fit(train)\n\nlr_predictions = lr_model.transform(test)\nlr_predictions.select(\"prediction\",\"time_to_reach_next\",\"features\").show(5)\n\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"time_to_reach_next\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\ntest_result = lr_model.evaluate(test)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+--------------------+\n        prediction|time_to_reach_next|            features|\n+------------------+------------------+--------------------+\n 144.9365588089422|               139|[-1059.0,7.0,0.0,...|\n106.25519479933428|                99|[-599.0,22.0,0.0,...|\n216.26384698715992|               220|[-551.0,0.0,0.0,3...|\n142.69643240253208|               140|[-469.0,5.0,0.0,3...|\n182.11879213151406|               200|[-456.0,1.0,0.0,3...|\n+------------------+------------------+--------------------+\nonly showing top 5 rows\n\nR Squared (R2) on test data = 0.807407\nRoot Mean Squared Error (RMSE) on test data = 18.855\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\ndt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'time_to_reach_next')\ndt_model = dt.fit(train)\ndt_predictions = dt_model.transform(test)\ndt_predictions.select(\"prediction\",\"time_to_reach_next\",\"features\").show(5)\n\ndt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % dt_evaluator.evaluate(dt_predictions))\n\ndt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % dt_evaluator.evaluate(dt_predictions))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'time_to_reach_next', maxIter=10)\ngbt_model = gbt.fit(train)\ngbt_predictions = gbt_model.transform(test)\ngbt_predictions.select(\"prediction\",\"time_to_reach_next\",\"features\").show(5)\n\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % gbt_evaluator.evaluate(gbt_predictions))\n\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % gbt_evaluator.evaluate(gbt_predictions))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+--------------------+\n        prediction|time_to_reach_next|            features|\n+------------------+------------------+--------------------+\n 414.5271412087056|               602|[-814.0,27.0,0.0,...|\n46.312766151198836|                60|[-737.0,27.0,20.0...|\n 72.51819277414972|                90|[-611.0,11.0,0.0,...|\n124.26515684680724|                96|[-536.0,0.0,0.0,2...|\n136.89124734579067|               130|[-529.0,12.0,-34....|\n+------------------+------------------+--------------------+\nonly showing top 5 rows\n\nR Squared (R2) on test data = 0.858423\nRoot Mean Squared Error (RMSE) on test data = 97.802\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# gbt_model.save('/user/hive/warehouse/streaming_gbt_model')\n\n# from pyspark.ml.regression import GBTRegressionModel\n# new_model = GBTRegressionModel.load('/user/hive/warehouse/super_gbt_model')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["# all_seg_freq = learn_df.groupBy('from_to').count().orderBy('count', ascending=False)\n# all_seg_freq.write.saveAsTable(\"all_segments_freq\")\nall_segments_freq = spark.read.options(header='true', inferSchema = 'true').load('/user/hive/warehouse/all_segments_freq')\npandas_freq = all_segments_freq.toPandas()\nfiltered_freqs = pandas_freq[pandas_freq['count'] > 100]\nprint('all segments in data: ', len(pandas_freq))\nprint('segments with over 100 records: ', len(filtered_freqs))\n\ntop_50 = [x for x in filtered_freqs['from_to'].values][:50]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">all segments in data:  127220\nsegments with over 100 records:  23482\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["display(filtered_freqs['count'].plot.hist(bins = 100, log = True, title = 'Segment Histogram'))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["filtered_freqs = filtered_freqs.sort_values(by=['count'], ascending  = False)\nfiltered_freqs['nums'] = [i for i in range(len(filtered_freqs))]\ndisplay(filtered_freqs.plot(x='nums',y ='count',legend = True, title = 'Segments Count', xlim = [-1000, 24000]))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n\ntotal_errors = 0\nfor seg in top_50:\n  df = learn_df[learn_df.from_to == seg] ##learn_df\n  numericCols = ['delay','vehicleSpeed','actualDelay','max_in_seg','min_in_seg','stdev_seg','distnce_between','dis_from_center','hour','speed_in_seg','latitude','longitude','mean_seg','prev_time']\n  assembler = VectorAssembler(inputCols = numericCols, outputCol=\"features\")\n  df = assembler.transform(df)\n  df = df.select([ 'features','time_to_reach_next'])\n  train, test = df.randomSplit([0.7, 0.3])\n  gbt = GBTRegressor(featuresCol = 'features', labelCol = 'time_to_reach_next', maxIter=10)\n  gbt_model = gbt.fit(train)\n  gbt_predictions = gbt_model.transform(test)\n  gbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\n  temp_error = gbt_evaluator.evaluate(gbt_predictions)\n  print(\"segment:\", seg, \"Root Mean Squared Error (RMSE) on test data = %g\" %temp_error)\n  total_errors += temp_error"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">segment: 1938-&gt;1939 Root Mean Squared Error (RMSE) on test data = 4.26711\nsegment: 1282-&gt;4456 Root Mean Squared Error (RMSE) on test data = 5.44507\nsegment: 270-&gt;335 Root Mean Squared Error (RMSE) on test data = 19.4399\nsegment: 4456-&gt;1284 Root Mean Squared Error (RMSE) on test data = 2.92286\nsegment: 335-&gt;4521 Root Mean Squared Error (RMSE) on test data = 10.3777\nsegment: 4640-&gt;4347 Root Mean Squared Error (RMSE) on test data = 6.79213\nsegment: 2002-&gt;1359 Root Mean Squared Error (RMSE) on test data = 12.7909\nsegment: 4415-&gt;301 Root Mean Squared Error (RMSE) on test data = 12.741\nsegment: 4348-&gt;4646 Root Mean Squared Error (RMSE) on test data = 8.61925\nsegment: 1352-&gt;1353 Root Mean Squared Error (RMSE) on test data = 4.95627\nsegment: 786-&gt;792 Root Mean Squared Error (RMSE) on test data = 7.8663\nsegment: 336-&gt;1279 Root Mean Squared Error (RMSE) on test data = 16.3085\nsegment: 14-&gt;15 Root Mean Squared Error (RMSE) on test data = 5.03155\nsegment: 264-&gt;6059 Root Mean Squared Error (RMSE) on test data = 16.6883\nsegment: 494-&gt;495 Root Mean Squared Error (RMSE) on test data = 7.02247\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'time_to_reach_next', maxIter=10)\ngbt_model = gbt.fit(train)\ngbt_predictions = gbt_model.transform(test)\ngbt_predictions.select(\"prediction\",\"time_to_reach_next\",\"features\").show(5)\n\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % gbt_evaluator.evaluate(gbt_predictions))\n\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % gbt_evaluator.evaluate(gbt_predictions))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+--------------------+\n        prediction|time_to_reach_next|            features|\n+------------------+------------------+--------------------+\n 180.3795855796577|               180|[-777.0,14.0,-18....|\n 146.4945091566682|               141|[-761.0,10.0,0.0,...|\n124.62552671826155|               121|[-742.0,12.0,-26....|\n115.68010358865442|               120|[-728.0,13.0,-17....|\n 95.99415134876382|                90|[-696.0,19.0,0.0,...|\n+------------------+------------------+--------------------+\nonly showing top 5 rows\n\nR Squared (R2) on test data = 0.942537\nRoot Mean Squared Error (RMSE) on test data = 20.2743\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["accidents_df = spark.read.csv(\"/FileStore/tables/accidents.csv\", header = True)\nevents_df = spark.read.csv(\"/FileStore/tables/events.csv\", header = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["accidents_df = accidents_df.withColumnRenamed('date', 'only_date')\naccidents_df = accidents_df.withColumn('date_time', accidents_df.date_time.cast('timestamp'))\naccidents_df = accidents_df.withColumn('lat', accidents_df.lat.cast('float'))\naccidents_df = accidents_df.withColumn('lon', accidents_df.lon.cast('float'))\n\naccidents_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+-------------------+---------+----------+\n_c0| only_date|          date_time|      lat|       lon|\n+---+----------+-------------------+---------+----------+\n  0|2018-09-10|2018-09-10 17:42:21| 53.37368|-6.2194905|\n  1|2018-09-10|2018-09-10 17:42:21|53.349766| -6.260273|\n  2|2018-09-10|2018-09-10 16:50:39|  53.5638|-6.2134967|\n  3|2018-09-10|2018-09-10 16:34:28| 53.35464|-6.2233615|\n  4|2018-09-10|2018-09-10 15:56:44| 53.35464|-6.2233615|\n+---+----------+-------------------+---------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["events_df = events_df.withColumnRenamed('date', 'only_date')\nevents_df = events_df.withColumn('lat', events_df.lat.cast('float'))\nevents_df = events_df.withColumn('lon', events_df.lon.cast('float'))\n\nevents_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+---------+----------+\n_c0| only_date|      lat|       lon|\n+---+----------+---------+----------+\n  0|2018-09-03|53.347122|-6.2756147|\n  1|2018-09-01|53.327686|  -6.23657|\n  2|2018-07-29|53.347122|-6.2756147|\n  3|2018-07-21|53.338566| -6.243588|\n  4|2018-07-21|53.338566| -6.243588|\n+---+----------+---------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_joined = learn_df.join(accidents_df,learn_df.only_date == accidents_df.only_date, how = 'left')\ndf_joined = df_joined.withColumn('distnce_from_acc',get_distance(f.col('lat'),f.col('lon'),f.col('latitude'),f.col('longitude')))\ndf_joined = df_joined.filter(df_joined.distnce_from_acc < 1000)\n\ndf_joined = df_joined.withColumn('time_from_acc', (df_joined.date.cast(\"bigint\") - df_joined.date_time.cast(\"bigint\"))  / 60)\ndf_joined = df_joined.filter(df_joined.time_from_acc < 1800).filter(df_joined.time_from_acc > -900)\n\ndf_joined = df_joined.drop_duplicates(subset=['id'])\ndf_joined = df_joined.withColumn('near_accident', lit(1))\ndf_joined = df_joined.withColumnRenamed('id','acc_id')\naccidints_ids = df_joined.select(['acc_id','near_accident']) #get all relevant id's\n\nlearn_with_accidents = learn_df.join(accidints_ids,learn_df.id == accidints_ids.acc_id, how = 'left_outer')\nlearn_with_accidents = learn_with_accidents.drop('acc_id')\nlearn_with_accidents = learn_with_accidents.fillna(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ndf = learn_with_accidents\nnumericCols = ['delay','vehicleSpeed','actualDelay','max_in_seg','min_in_seg','stdev_seg','distnce_between','dis_from_center','hour','speed_in_seg','latitude','longitude','mean_seg','prev_time','near_accident']\nassembler = VectorAssembler(inputCols = numericCols, outputCol=\"features\")\ndf = assembler.transform(df)\ndf = df.select([ 'features','time_to_reach_next'])\ntrain, test = df.randomSplit([0.7, 0.3])\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'time_to_reach_next', maxIter=10)\ngbt_model = gbt.fit(train)\ngbt_predictions = gbt_model.transform(test)\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\ntemp_error = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" %temp_error)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Root Mean Squared Error (RMSE) on test data = 101.615\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ndf_joined = learn_df.join(events_df,learn_df.only_date == events_df.only_date, how = 'left')\n\ndf_joined = df_joined.withColumn('distnce_from_eve',get_distance(f.col('lat'),f.col('lon'),f.col('latitude'),f.col('longitude')))\ndf_joined = df_joined.filter(df_joined.distnce_from_eve < 1000)\n\ndf_joined = df_joined.drop_duplicates(subset=['id'])\ndf_joined = df_joined.withColumn('near_event', lit(1))\ndf_joined = df_joined.withColumnRenamed('id','eve_id')\nevents_ids = df_joined.select(['eve_id','near_event']) #get all relevant id's\n\nlearn_with_events = learn_df.join(events_ids,learn_df.id == events_ids.eve_id, how = 'left_outer')\nlearn_with_events = learn_with_events.drop('eve_id')\nlearn_with_events = learn_with_events.fillna(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ndf = learn_with_events\nnumericCols = ['delay','vehicleSpeed','actualDelay','max_in_seg','min_in_seg','stdev_seg','distnce_between','dis_from_center','hour','speed_in_seg','latitude','longitude','mean_seg','prev_time','near_event']\nassembler = VectorAssembler(inputCols = numericCols, outputCol=\"features\")\ndf = assembler.transform(df)\ndf = df.select([ 'features','time_to_reach_next'])\ntrain, test = df.randomSplit([0.7, 0.3])\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'time_to_reach_next', maxIter=10)\ngbt_model = gbt.fit(train)\ngbt_predictions = gbt_model.transform(test)\ngbt_evaluator = RegressionEvaluator(labelCol=\"time_to_reach_next\", predictionCol=\"prediction\", metricName=\"rmse\")\ntemp_error = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" %temp_error)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Root Mean Squared Error (RMSE) on test data = 102.425\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["segment_data = learn_df.groupBy('from_to').mean('mean_seg','stdev_seg','max_in_seg','min_in_seg','prev_time','distnce_between','dis_from_center','speed_in_seg')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["segment_data = segment_data.withColumnRenamed('avg(mean_seg)', 'mean_seg')\nsegment_data = segment_data.withColumnRenamed('avg(stdev_seg)', 'stdev_seg')\nsegment_data = segment_data.withColumnRenamed('avg(max_in_seg)', 'max_in_seg')\nsegment_data = segment_data.withColumnRenamed('avg(min_in_seg)', 'min_in_seg')\nsegment_data = segment_data.withColumnRenamed('avg(prev_time)', 'prev_time')\nsegment_data = segment_data.withColumnRenamed('avg(distnce_between)', 'distnce_between')\nsegment_data = segment_data.withColumnRenamed('avg(dis_from_center)', 'dis_from_center')\nsegment_data = segment_data.withColumnRenamed('avg(speed_in_seg)', 'speed_in_seg')\n\nsegment_data.write.saveAsTable(\"segment_data\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37}],"metadata":{"name":"project_1","notebookId":1629096087198190},"nbformat":4,"nbformat_minor":0}
